{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Claim Correction: Evidence from the Russia-Ukraine Conflict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social media platforms enable the rapid and widespread dissemination of new claims, while corrective responses often lag in both timing and visibility. This paper presents a system for real-time identification and response to contentious content. The system maintains a database of pre-compiled claims and corresponding corrections. It analyzes incoming social media posts, news articles, and other online content to extract embedded claims, retrieve the appropriate corrections, and automatically generate appropriate responses. We evaluate our approach using a corpus of 313 social media posts from Russian state propaganda outlets. In 76\\% of cases, our systemâ€™s responses were judged more effective at challenging the content than the crowdsourced human alternative (Twitter Community Notes). These results demonstrate the potential for automated approaches to provide timely and scalable interventions in contentious online discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "# Helper functions\n",
    "from libs.utils import save_dict_as_json, load_dict_from_json\n",
    "from libs.strings import remove_links\n",
    "from libs.batch_processor import parallel_process_batches, parallel_embed_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "filepath_entities_of_interest = './Data/entities-of-interest.json'\n",
    "filepath_tweet_id_to_url = './Data/tweet-id-to-url.json'\n",
    "filepath_tweets = './Data/tweets.json'\n",
    "\n",
    "# Community notes\n",
    "filepath_community_notes_raw = './Data/notes-00000.tsv'\n",
    "filepath_community_notes_status = './Data/noteStatusHistory-00000.tsv'\n",
    "filepath_community_notes_filtered = './Data/community-notes.json'\n",
    "\n",
    "# Analyses\n",
    "filepath_dyads = './Analyses/1-dyads.json'\n",
    "filepath_claims = './Analyses/4-claims.json'\n",
    "filepath_claims_final = './Analyses/4-claims-final.json'\n",
    "filepath_claims_embeddings = './Analyses/5-claims-embeddings.json'\n",
    "filepath_counterclaims = './Analyses/6-counterclaims.json'\n",
    "filepath_database = './Analyses/7-database.json'\n",
    "filepath_cached_requests = './Analyses/8-cached-requests.json'\n",
    "filepath_responses = './Analyses/9-responses.json'\n",
    "filepath_evaluations = './Analyses/10-evaluations.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mise-en-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entities of interest\n",
    "with open(filepath_entities_of_interest, 'r') as f:\n",
    "    entities_of_interest = json.load(f)\n",
    "\n",
    "# Lexicon\n",
    "lexicon = [\n",
    "    # General\n",
    "    'military', 'missile', 'nuclear', \n",
    "    \n",
    "    # Ukraine\n",
    "    'ukraine', 'ukrainian', 'ukrainians', 'kyiv',\n",
    "\n",
    "    # Russia\n",
    "    'russia', 'russian', 'russians', 'moscow', 'kremlin',\n",
    "    \n",
    "    # People\n",
    "    'putin', 'zelensky', 'zelenskyy', 'zakharova', 'lavrov', 'poroshenko', 'yanukovych', 'yanukovich', 'klitschko',\n",
    "\n",
    "    # Ideologies\n",
    "    'nazi', 'nazism', 'nazis', 'neo-nazi',\n",
    "\n",
    "    # Organizations\n",
    "    'nato', 'osce', 'red army', 'cia', \n",
    "\n",
    "    # Places\n",
    "    'donbas', 'donbass', 'donetsk', 'luhansk', 'crimea', 'kharkiv', \n",
    "    'kherson', 'odessa', 'zaporizhzhia', 'zaporizhzhya', 'zaporizhzhya', \n",
    "    'sevastopol', 'simferopol', 'mariupol', 'mariupol', \n",
    "    \n",
    "    # Other countries\n",
    "    'poland',\n",
    "]\n",
    "\n",
    "# Date Window\n",
    "MIN_DATE = '2022-02-23'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Community Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download 'Notes data' and 'Note status history data' from [here](https://x.com/i/communitynotes/download-data) into Data/ folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we filter the community notes data to only include notes that are related to the Russia-Ukraine conflict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Filter by Review Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove notes who aren't in the 'CURRENTLY_RATED_HELPFUL' status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load community notes\n",
    "df_raw = pd.read_csv(filepath_community_notes_raw, sep='\\t')\n",
    "df_status = pd.read_csv(filepath_community_notes_status, sep='\\t')\n",
    "\n",
    "# Log\n",
    "print(f\"Total notes: {len(df_raw):,}\")\n",
    "\n",
    "# Config\n",
    "columns_raw = [\n",
    "    'noteId',\n",
    "    'createdAtMillis',\n",
    "    'tweetId',\n",
    "    'summary'\n",
    "]\n",
    "columns_status = [\n",
    "    'noteId',\n",
    "    'currentStatus'\n",
    "]\n",
    "status_to_remove = [\n",
    "    'CURRENTLY_RATED_NOT_HELPFUL',\n",
    "    'NEEDS_MORE_RATINGS'\n",
    "]\n",
    "\n",
    "# Remove all but the columns of interest\n",
    "df_raw = df_raw[columns_raw]\n",
    "df_status = df_status[columns_status]\n",
    "\n",
    "# Convert tweet ids to strings\n",
    "df_raw['tweetId'] = df_raw['tweetId'].astype(str)\n",
    "df_raw['noteId'] = df_raw['noteId'].astype(str)\n",
    "df_status['noteId'] = df_status['noteId'].astype(str)\n",
    "\n",
    "# Join dataframes\n",
    "df_raw = df_raw.merge(df_status, on='noteId', how='inner')\n",
    "\n",
    "# Remove all rows with status to remove\n",
    "df_raw = df_raw[~df_raw['currentStatus'].isin(status_to_remove)]\n",
    "\n",
    "# Save as json\n",
    "df_raw.to_json(filepath_community_notes_filtered, orient='records', indent=4, force_ascii=False)\n",
    "\n",
    "# Log \n",
    "print(f\"Total notes after filtering: {len(df_raw):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filter by Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text into words using only the re library.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation and non-alphanumeric characters, replace with spaces\n",
    "    text = re.sub(r'[\\W_]+', ' ', text)\n",
    "    \n",
    "    # Split by whitespace to get tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load community notes\n",
    "with open(filepath_community_notes_filtered, 'r') as file:\n",
    "    community_notes = json.load(file)\n",
    "\n",
    "# Filter using the lexicon\n",
    "community_notes_filtered = []\n",
    "\n",
    "# Iterate over\n",
    "for note in tqdm(community_notes):\n",
    "\n",
    "    # Unpack\n",
    "    text = note['summary']\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = set(tokenize(text))\n",
    "\n",
    "    # Check if any token is in the lexicon\n",
    "    if not any(token in lexicon for token in tokens):\n",
    "        continue\n",
    "\n",
    "    # Append\n",
    "    community_notes_filtered.append(note)\n",
    "\n",
    "# Save\n",
    "with open(filepath_community_notes_filtered, 'w') as file:\n",
    "    json.dump(community_notes_filtered, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Log\n",
    "print(f\"Total notes after lexicon filtering: {len(community_notes_filtered):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Filter by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load community notes\n",
    "with open(filepath_community_notes_filtered, 'r') as file:\n",
    "    community_notes = json.load(file)\n",
    "\n",
    "# Initialize\n",
    "community_notes_filtered = []\n",
    "\n",
    "# Iterate over\n",
    "for note in tqdm(community_notes):\n",
    "\n",
    "    # Unpack\n",
    "    text = note['summary']\n",
    "\n",
    "    # If not English skip\n",
    "    if not is_english(text): continue\n",
    "\n",
    "    # Append\n",
    "    community_notes_filtered.append(note)\n",
    "\n",
    "# Save\n",
    "with open(filepath_community_notes_filtered, 'w') as file:\n",
    "    json.dump(community_notes_filtered, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Log\n",
    "print(f\"Total notes after language filtering: {len(community_notes_filtered):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Filter by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to millis\n",
    "MIN_DATE = datetime.strptime(MIN_DATE, '%Y-%m-%d').timestamp() * 1000\n",
    "\n",
    "# Load community notes\n",
    "with open(filepath_community_notes_filtered, 'r') as file:\n",
    "    community_notes = json.load(file)\n",
    "\n",
    "# Initialize\n",
    "community_notes_filtered = []\n",
    "\n",
    "# Iterate over\n",
    "for note in tqdm(community_notes):\n",
    "\n",
    "    # Unpack\n",
    "    date = note['createdAtMillis']\n",
    "\n",
    "    # If date is too early skip\n",
    "    if date < MIN_DATE: continue\n",
    "\n",
    "    # Append\n",
    "    community_notes_filtered.append(note)\n",
    "\n",
    "# Save\n",
    "with open(filepath_community_notes_filtered, 'w') as file:\n",
    "    json.dump(community_notes_filtered, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Log\n",
    "print(f\"Total notes after date filtering: {len(community_notes_filtered):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the date span of the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load community notes\n",
    "with open(filepath_community_notes_filtered, 'r') as file:\n",
    "    community_notes = json.load(file)\n",
    "\n",
    "# Initialize\n",
    "dates_arr = []\n",
    "\n",
    "# Iterate over\n",
    "for note in community_notes:\n",
    "\n",
    "    # Convert to yyyy-mm-dd\n",
    "    date = note['createdAtMillis']\n",
    "    date = datetime.fromtimestamp(date / 1000).strftime('%Y-%m-%d')\n",
    "    dates_arr.append(date)\n",
    "\n",
    "# Sort\n",
    "dates_arr = sorted(dates_arr)\n",
    "\n",
    "# Get the min/max\n",
    "min_date = dates_arr[0]\n",
    "max_date = dates_arr[-1]\n",
    "\n",
    "# Log\n",
    "print(f\"Min date: {min_date}\")\n",
    "print(f\"Max date: {max_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Filter by account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notes do not have the account information by default. Tweet ID to URL was obtained from the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mapping of tweet id to url\n",
    "with open(filepath_tweet_id_to_url, 'r') as file:\n",
    "    tweet_id_to_url = json.load(file)\n",
    "\n",
    "# Load community notes\n",
    "with open(filepath_community_notes_filtered, 'r') as file:\n",
    "    community_notes = json.load(file)\n",
    "\n",
    "# Map handles to entities\n",
    "entity_by_handle = {}\n",
    "for entity in entities_of_interest:\n",
    "    for channel in entity['Channels']:\n",
    "        if channel['Platform'] != 'Twitter': continue\n",
    "        entity_by_handle[channel['Handle'].lower()] = entity\n",
    "\n",
    "# Initialize\n",
    "community_notes_filtered = []\n",
    "\n",
    "# Iterate over\n",
    "for note in tqdm(community_notes):\n",
    "\n",
    "    # Unpack\n",
    "    tweet_id = note['tweetId']\n",
    "\n",
    "    # Skip if no url\n",
    "    if tweet_id not in tweet_id_to_url: continue\n",
    "\n",
    "    # Get url\n",
    "    url = tweet_id_to_url[tweet_id]\n",
    "\n",
    "    # Check if any handle is in the url\n",
    "    handle = url.split('/')[-3].lower()\n",
    "\n",
    "    # Check if handle is in the entities of interest\n",
    "    if handle not in entity_by_handle: continue\n",
    "\n",
    "    # Add metadata\n",
    "    note['entity_id'] = entity_by_handle[handle]['ID']\n",
    "    note['url'] = url\n",
    "\n",
    "    # Append\n",
    "    community_notes_filtered.append(note)\n",
    "\n",
    "# Save\n",
    "with open(filepath_community_notes_filtered, 'w') as file:\n",
    "    json.dump(community_notes_filtered, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Log\n",
    "print(f\"Total notes after filtering: {len(community_notes_filtered):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the final breakdown of notes by account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load community notes\n",
    "with open(filepath_community_notes_filtered, 'r') as file:\n",
    "    community_notes = json.load(file)\n",
    "\n",
    "# Map entity_id to entity name\n",
    "entity_name_by_id = {}\n",
    "for entity in entities_of_interest:\n",
    "    entity_name_by_id[entity['ID']] = entity['Name']\n",
    "\n",
    "# Initialize\n",
    "entity_count = {}\n",
    "for entity in entities_of_interest:\n",
    "    entity_count[entity['ID']] = 0\n",
    "\n",
    "# Iterate over\n",
    "for note in community_notes:\n",
    "    entity_id = note['entity_id']\n",
    "    entity_count[entity_id] += 1\n",
    "\n",
    "# Remove entities with no notes\n",
    "entity_count = { k: v for k, v in entity_count.items() if v > 0 }\n",
    "\n",
    "# Sort\n",
    "entity_count = { k: v for k, v in sorted(entity_count.items(), key=lambda item: item[1], reverse=True) }\n",
    "\n",
    "# Log\n",
    "for entity_id, count in entity_count.items():\n",
    "    print(f\"{entity_name_by_id[entity_id]}: {count:,}\")\n",
    "print(f\"Total notes: {sum(entity_count.values()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tweet-Note Dyads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notes do not have the original tweeet by default. Tweets were obtained from the Twitter API. We combine them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(filepath_tweets, 'r') as file:\n",
    "    tweets = json.load(file)\n",
    "with open(filepath_community_notes_filtered, 'r') as file:\n",
    "    community_notes = json.load(file)\n",
    "\n",
    "# Map tweet id to tweet\n",
    "tweet_by_id = {}\n",
    "for tweet in tweets:\n",
    "    tweet_by_id[tweet['tweetId']] = tweet\n",
    "\n",
    "# Initialize\n",
    "dyads = []\n",
    "\n",
    "# Iterate over notes\n",
    "for note in community_notes:\n",
    "\n",
    "    # Unpack\n",
    "    tweet_id = note['tweetId']\n",
    "    url = note['url']\n",
    "    note_text = note['summary']\n",
    "    entity_id = note['entity_id']\n",
    "\n",
    "    # Skip if no tweet\n",
    "    if tweet_id not in tweet_by_id: continue\n",
    "\n",
    "    # Get tweet\n",
    "    tweet = tweet_by_id[tweet_id]\n",
    "    tweet_text = tweet['text']\n",
    "\n",
    "\n",
    "    # Append\n",
    "    dyads.append({\n",
    "        'tweetId': tweet_id,\n",
    "        'url': url,\n",
    "        'entity_id': entity_id,\n",
    "        'note': note_text,\n",
    "        'tweet': tweet_text,\n",
    "    })\n",
    "\n",
    "# Log\n",
    "print(f\"Total dyads: {len(dyads):,}\")\n",
    "\n",
    "# Save\n",
    "with open(filepath_dyads, 'w') as file:\n",
    "    json.dump(dyads, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Claim Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an LLM, extract the claims from each filtered document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompt_claims(text):\n",
    "\n",
    "    # Clean\n",
    "    text = remove_links(text)\n",
    "\n",
    "    return f\"\"\"\n",
    "DOCUMENT:\n",
    "\"{text}\"\n",
    "\n",
    "TASK:\n",
    "Identify all claims in the document. A claim is a statement that:\n",
    "- Asserts or implies something as true, factual, or plausible (e.g., \"X happened,\" \"Y is true,\" or \"Z will occur\").\n",
    "- Takes a position (e.g., supports or opposes something).\n",
    "- Shows a connection, causation, or relationship (e.g., \"Could X have caused Y?\").\n",
    "- Proposes an explanation, hypothesis, or prediction.\n",
    "\n",
    "GUIDELINES:\n",
    "- Claims must be concise and self-contained.\n",
    "- Exclude unnecessary details unless essential for clarity.\n",
    "- Focus each claim on a single idea.\n",
    "- Keep claims objective and based only on the document content.\n",
    "\n",
    "OUTPUT:\n",
    "Return the main claims as a JSON array. For example:\n",
    "claims: [\n",
    "    \"NATO provoked the war\",\n",
    "    \"Neo-Nazis are in the Ukrainian government\",\n",
    "    ...\n",
    "]\n",
    "\n",
    "If the document does not contain any claims, return an empty list: [].\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dyads\n",
    "with open(filepath_dyads, 'r') as file:\n",
    "    dyads = json.load(file)\n",
    "\n",
    "# Initialize/Load Results\n",
    "claims_dict = {}\n",
    "if os.path.exists(filepath_claims):\n",
    "    claims_dict = load_dict_from_json(filepath_claims)\n",
    "\n",
    "# Initialize Tasks\n",
    "tasks = []\n",
    "\n",
    "# Iterate over\n",
    "for document in dyads:\n",
    "\n",
    "    # Skip if already annotated\n",
    "    key = (document['url'],)\n",
    "    if key in claims_dict and claims_dict[key] is not None: continue\n",
    "    \n",
    "    # Compose prompt\n",
    "    prompt = compose_prompt_claims(document['tweet'])\n",
    "\n",
    "    # Append\n",
    "    tasks.append((key, prompt))\n",
    "\n",
    "# Log number of tasks\n",
    "print(f'Found {len(tasks):,} tasks to process.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_MAP = None\n",
    "NUM_WORKERS = 10\n",
    "FILEPATH = filepath_claims\n",
    "JSON_MODE = True\n",
    "MODEL_LARGE = True\n",
    "\n",
    "# Run the function\n",
    "parallel_process_batches(\n",
    "    tasks=tasks, \n",
    "    filepath=FILEPATH, \n",
    "    results_map=RESULTS_MAP, \n",
    "    json_mode=JSON_MODE,\n",
    "    model_large=MODEL_LARGE, \n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and format all claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load claims\n",
    "claims_dict = {}\n",
    "if os.path.exists(filepath_claims):\n",
    "    claims_dict = load_dict_from_json(filepath_claims)\n",
    "\n",
    "# Iterate over and format\n",
    "for key in claims_dict.keys():\n",
    "    if claims_dict[key] is None: continue\n",
    "\n",
    "    # Initialize\n",
    "    claims = set()\n",
    "\n",
    "    # Initial format { 'claims': [ ... ] } (due to llm processing)\n",
    "    if 'claims' in claims_dict[key]:\n",
    "        claims = set(claims_dict[key]['claims'])\n",
    "\n",
    "    else:\n",
    "        claims = set(claims_dict[key])\n",
    "\n",
    "    # Remove '.' at the end\n",
    "    claims = set([ claim[:-1].strip() if claim[-1] == '.' else claim.strip() for claim in claims ])\n",
    "\n",
    "    # Update\n",
    "    claims_dict[key] = list(claims)\n",
    "\n",
    "# Save\n",
    "save_dict_as_json(claims_dict, filepath_claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Claim Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent each claim as a vector using a text embedding model. This vector representation captures the semantic meaning of the claim, enabling similarity comparisons and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load claims\n",
    "claims_dict = load_dict_from_json(filepath_claims)\n",
    "\n",
    "# Collect all claims across documents\n",
    "claims = set()\n",
    "for values in claims_dict.values():\n",
    "    claims.update(values)\n",
    "\n",
    "# Initialize / Load\n",
    "embeddings_dict = {}\n",
    "if os.path.exists(filepath_claims_embeddings):\n",
    "    embeddings_dict = load_dict_from_json(filepath_claims_embeddings)\n",
    "\n",
    "# Create tasks\n",
    "tasks = []\n",
    "for claim in claims:\n",
    "    \n",
    "    # Skip if already processed\n",
    "    key = (claim,)\n",
    "    if key in embeddings_dict and embeddings_dict[key] is not None: continue\n",
    "    \n",
    "    # Add to tasks\n",
    "    tasks.append((key, claim))\n",
    "\n",
    "# Log\n",
    "print(f'Found {len(tasks):,} tasks to process.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "parallel_embed_batches(tasks, filepath_claims_embeddings, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Counter-Claim Generation (FOR PROOF OF CONCEPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STRICTLY FOR THE INITIAL PROOF OF CONCEPT. Usually this would be done by human expert. \n",
    "\n",
    "For each extracted claim, use an LLM to generate a counterclaim that employs a **fact-based strategy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompt_generate_counterclaim(claim): \n",
    "    suffix = '\\n{\\n   \"claim\": \"' + claim + '\",\\n   \"label\": \"True/False\",\\n   \"evidence\": \"Short, factual summary with hard evidence (cite only key sources).\"\\n}'\n",
    "    return f\"\"\"\n",
    "CLAIM:\n",
    "\"{claim}\"\n",
    "\n",
    "TASK:\n",
    "Verify the factual accuracy of the claim. Provide a short explanation supported by concise and hard evidence.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Analyze the claim: Identify the key assertions and context.\n",
    "2. Research evidence: Use only credible sources (e.g., academic studies, verified news, official records).\n",
    "3. Evaluate reasoning: Identify any fallacies, biases, or misinformation.\n",
    "4. Determine accuracy: Label the claim as \"True\" or \"False.\" Justify your label with brief, precise, and verifiable evidence (no lengthy explanations).\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return the analysis as a structured JSON object in this format:\n",
    "\"\"\".strip() + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "tasks = []\n",
    "\n",
    "# Load claims\n",
    "with open(filepath_claims_final, 'r') as file:\n",
    "    claims = json.load(file)\n",
    "\n",
    "# Load claims results\n",
    "counterclaims_dict = {}\n",
    "if os.path.exists(filepath_counterclaims):\n",
    "    counterclaims_dict = load_dict_from_json(filepath_counterclaims)\n",
    "\n",
    "# Iterate over\n",
    "for claim in claims:\n",
    "\n",
    "    # Skip if already annotated\n",
    "    key = (claim,)\n",
    "    if key in counterclaims_dict and counterclaims_dict[key] is not None: continue\n",
    "\n",
    "    # Compose prompt\n",
    "    prompt = compose_prompt_generate_counterclaim(claim)\n",
    "\n",
    "    # Append\n",
    "    tasks.append((key, prompt))\n",
    "\n",
    "# Log number of tasks\n",
    "print(f'Found {len(tasks):,} tasks to process.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_MAP = None\n",
    "NUM_WORKERS = 10\n",
    "FILEPATH = filepath_counterclaims\n",
    "JSON_MODE = True\n",
    "MODEL_LARGE = True\n",
    "\n",
    "# Run the function\n",
    "parallel_process_batches(\n",
    "    tasks=tasks, \n",
    "    filepath=FILEPATH, \n",
    "    results_map=RESULTS_MAP, \n",
    "    json_mode=JSON_MODE,\n",
    "    model_large=MODEL_LARGE, \n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and format all counterclaims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "counterclaims_dict = load_dict_from_json(filepath_counterclaims)\n",
    "\n",
    "# Iterate over and format\n",
    "for key in counterclaims_dict.keys():\n",
    "    if 'claim' in counterclaims_dict[key]:\n",
    "        del counterclaims_dict[key]['claim']\n",
    "        \n",
    "# Save\n",
    "save_dict_as_json(counterclaims_dict, filepath_counterclaims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Claim-Counterclaim Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A database pairing each claim with its corresponding counterclaim. Additionally, each claim is stored with its vector representation and text to facilitate retrieval during discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final claims\n",
    "with open(filepath_claims_final, 'r') as file:\n",
    "    claims = json.load(file)\n",
    "\n",
    "# Load embeddings\n",
    "embeddings_dict = load_dict_from_json(filepath_claims_embeddings)\n",
    "embeddings_dict = { key: value for key, value in embeddings_dict.items() if key[0] in claims }\n",
    "\n",
    "# Load claims results\n",
    "counterclaims_results = load_dict_from_json(filepath_counterclaims)\n",
    "counterclaims_results = { key: value for key, value in counterclaims_results.items() if key[0] in claims }\n",
    "\n",
    "# Initialize\n",
    "database = []\n",
    "\n",
    "# Iterate over\n",
    "for claim in claims:\n",
    "\n",
    "    # Get key\n",
    "    key = (claim,)\n",
    "    embedding = embeddings_dict[key]\n",
    "    counterclaim = counterclaims_results[key]\n",
    "    \n",
    "    # Append\n",
    "    database.append({\n",
    "        'claim': claim,\n",
    "        'counterclaim': counterclaim['evidence'],\n",
    "        'embedding': embedding\n",
    "    })\n",
    "\n",
    "# Save\n",
    "with open(filepath_database, 'w') as file:\n",
    "    json.dump(database, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Log\n",
    "print(f\"Total claims in database: {len(database):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Document Counterclaim Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair new documents with relevant counterclaims for response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.database import pipeline_lookup_counterclaims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dyads\n",
    "with open(filepath_dyads, 'r') as file:\n",
    "    dyads = json.load(file)\n",
    "\n",
    "# Initialize\n",
    "counterclaims_dict = {}\n",
    "if os.path.exists(filepath_cached_requests):\n",
    "    with open(filepath_cached_requests, 'r') as file:\n",
    "        counterclaims_dict = json.load(file)\n",
    "\n",
    "# Iterate over\n",
    "for document in dyads:\n",
    "\n",
    "    # If already processed, skip \n",
    "    if document['url'] in counterclaims_dict:\n",
    "        continue\n",
    "\n",
    "    # Unpack\n",
    "    id_str = document['url']\n",
    "    text = document['tweet']\n",
    "\n",
    "    # Retrieve counterclaims\n",
    "    counterclaims = pipeline_lookup_counterclaims(text, filepath_database=filepath_database)    \n",
    "\n",
    "    # Store\n",
    "    counterclaims_dict[id_str] = counterclaims\n",
    "\n",
    "    # Save\n",
    "    with open(filepath_cached_requests, 'w') as file:\n",
    "        json.dump(counterclaims_dict, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, generate responses using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of different responses to generate per method per document\n",
    "NUM_RESPONSES_PER_METHOD = 1\n",
    "\n",
    "# Different methods to generate responses\n",
    "RESPONSE_GENERATION_CATEGORIES = ['control', 'generic', 'RAG', 'community_note', 'community_note_RAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RULES = \"\"\"\n",
    "RULES:\n",
    "- Brevity: Responses must be brief and focused.\n",
    "- Neutrality: Avoid emotionally charged, inflammatory, or biased language. Maintain a factual and objective tone.\n",
    "- Accuracy: Ensure correct spelling and consistent use of established names or terminology (e.g., countries, organizations, historical figures).\n",
    "- Formatting: Do not use any formatting or special characters.\n",
    "- Author's Stance: Consider the provided author's stance when crafting the response, but never mention it explicitly.\n",
    "- Final Response Only: Provide only the final response. Do not include reasoning, intermediate steps, or explanations in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def compose_prompt_control(text):\n",
    "    return f\"\"\"\n",
    "DOCUMENT:\n",
    "\"{text}\"\n",
    "\n",
    "TASK:\n",
    "Write a short response that superficially challenges the main point of the document. Focus only on surface-level issues and avoid providing strong evidence or detailed reasoning.\n",
    "\n",
    "{RULES}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def compose_prompt_generic(text):\n",
    "    return f\"\"\"\n",
    "DOCUMENT:\n",
    "\"{text}\"\n",
    "\n",
    "TASK:\n",
    "Write a clear, concise, and constructive response that challenges the main point of the document.\n",
    "\n",
    "{RULES}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def compose_prompt_RAG(text, counterclaims):\n",
    "    random.shuffle(counterclaims)   # Randomize order of counter claims for different responses\n",
    "    counterclaims_str = '\\n'.join([ f'- \"{counterclaim}\"' for counterclaim in counterclaims ])\n",
    "    return f\"\"\"\n",
    "DOCUMENT:\n",
    "\"{text}\"\n",
    "\n",
    "RELEVANT INFORMATION:\n",
    "{counterclaims_str}\n",
    "\n",
    "TASK:\n",
    "Using the relevant information provided, write a clear, concise, and constructive response that challenges the main point of the document.\n",
    "\n",
    "{RULES}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dyads\n",
    "with open(filepath_dyads, 'r') as file:\n",
    "    dyads = json.load(file)\n",
    "\n",
    "# Initialize\n",
    "tasks = []\n",
    "\n",
    "# Load counterclaims\n",
    "with open(filepath_counterclaims, 'r') as file:\n",
    "    counterclaims_dict = json.load(file)\n",
    "\n",
    "# Load responses\n",
    "responses_dict = {}\n",
    "if os.path.exists(filepath_responses):\n",
    "    responses_dict = load_dict_from_json(filepath_responses)\n",
    "\n",
    "# Iterate over\n",
    "for document in dyads:\n",
    "\n",
    "    # Unpack\n",
    "    id_str = document['url']\n",
    "    tweet_id = document['tweetId']\n",
    "    text = document['tweet']\n",
    "    note = document['note']\n",
    "\n",
    "    # If no counterclaims, skip\n",
    "    if id_str not in counterclaims_dict:\n",
    "        print(f\"Skipping {id_str} due to missing counterclaims.\")\n",
    "        continue\n",
    "\n",
    "    # Load counterclaims\n",
    "    counterclaims = counterclaims_dict[id_str]\n",
    "\n",
    "    # Iterate over\n",
    "    for idx in range(NUM_RESPONSES_PER_METHOD):\n",
    "\n",
    "        # Response #1: Control\n",
    "        key_1 = (id_str, 'control', idx)\n",
    "        response_1 = compose_prompt_control(text)\n",
    "        if key_1 not in responses_dict or responses_dict[key_1] is None:\n",
    "            tasks.append((key_1, response_1))\n",
    "\n",
    "        # Response #2: Generic LLM response\n",
    "        key_2 = (id_str, 'generic', idx)\n",
    "        response_2 = compose_prompt_generic(text)\n",
    "        if key_2 not in responses_dict or responses_dict[key_2] is None:\n",
    "            tasks.append((key_2, response_2))\n",
    "        \n",
    "        # Response #3: Retrieval-Augmented Generation\n",
    "        key_3 = (id_str, 'RAG', idx)\n",
    "        response_3 = compose_prompt_RAG(text, counterclaims)\n",
    "        if key_3 not in responses_dict or responses_dict[key_3] is None:\n",
    "            tasks.append((key_3, response_3))\n",
    "\n",
    "        # Response #4: Community Note\n",
    "        #   manually added...\n",
    "\n",
    "        # Response #5: Community Note with RAG\n",
    "        key_5 = (id_str, 'community_note_RAG', idx)\n",
    "        response_5 = compose_prompt_RAG(text, [note])\n",
    "        if key_5 not in responses_dict or responses_dict[key_5] is None:\n",
    "            tasks.append((key_5, response_5))\n",
    "    \n",
    "# Log\n",
    "print(f\"Generated {len(tasks):,} tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_MAP = None\n",
    "NUM_WORKERS = 10\n",
    "FILEPATH = filepath_responses\n",
    "JSON_MODE = False\n",
    "MODEL_LARGE = True\n",
    "\n",
    "# Run the function\n",
    "parallel_process_batches(\n",
    "    tasks=tasks,\n",
    "    filepath=FILEPATH, \n",
    "    results_map=RESULTS_MAP, \n",
    "    json_mode=JSON_MODE,\n",
    "    model_large=MODEL_LARGE, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation (FOR PROOF OF CONCEPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories\n",
    "RESPONSE_GENERATION_CATEGORIES = ['control', 'generic', 'RAG', 'community_note', 'community_note_RAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompt_ranking_effectiveness(text, response_a, response_b):\n",
    "\n",
    "    # Clean\n",
    "    text = remove_links(text)\n",
    "    response_a = remove_links(response_a)\n",
    "    response_b = remove_links(response_b)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are evaluating two responses to the following document. Your task is to determine which response more effectively challenges the main point of the document.\n",
    "\n",
    "Criteria for an Effective Response:\n",
    "- Maintains a neutral and factual tone.\n",
    "- Directly addresses the main point of the document.\n",
    "- Uses relevant facts and evidence to support its argumentation.\n",
    "\n",
    "DOCUMENT:\n",
    "\"{text}\"\n",
    "\n",
    "RESPONSE A:\n",
    "\"{response_a}\"\n",
    "\n",
    "RESPONSE B:\n",
    "\"{response_b}\"\n",
    "\n",
    "QUESTION: \n",
    "Which response (A or B) is more effective at challenging the main point of the document?\n",
    "\n",
    "Respond ONLY with a single letter: A or B. Do not add any explanations or additional text.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dyads\n",
    "with open(filepath_dyads, 'r') as file:\n",
    "    dyads = json.load(file)\n",
    "\n",
    "# Initialize\n",
    "tasks = []\n",
    "\n",
    "# Load responses\n",
    "responses_dict = load_dict_from_json(filepath_responses)\n",
    "\n",
    "# Load evaluation results\n",
    "evaluation_dict = {}\n",
    "if os.path.exists(filepath_evaluations):\n",
    "    evaluation_dict = load_dict_from_json(filepath_evaluations)\n",
    "\n",
    "# Iterate over\n",
    "for document in dyads:\n",
    "\n",
    "    # Unpack\n",
    "    id_str = document['url']\n",
    "    text = document['tweet']\n",
    "\n",
    "    # Generate pairs excluding same-category comparisons\n",
    "    pairs = []\n",
    "\n",
    "    # Iterate only over unique category combinations\n",
    "    for i, category_A in enumerate(RESPONSE_GENERATION_CATEGORIES):\n",
    "        for j, category_B in enumerate(RESPONSE_GENERATION_CATEGORIES[i + 1:], start=i + 1):  # Avoid duplicate or reversed pairs\n",
    "            \n",
    "            # Get responses\n",
    "            responses_A = []\n",
    "            responses_B = []\n",
    "            for i in range(10):\n",
    "                key_a = (id_str, category_A, i)\n",
    "                key_b = (id_str, category_B, i)\n",
    "                if key_a in responses_dict:\n",
    "                    responses_A.append(responses_dict[key_a])\n",
    "                if key_b in responses_dict:\n",
    "                    responses_B.append(responses_dict[key_b])\n",
    "            \n",
    "            for response_A in responses_A:\n",
    "                for response_B in responses_B:\n",
    "                    pairs.append((response_A, response_B))\n",
    "\n",
    "    # Iterate over\n",
    "    for response_A, response_B in pairs:\n",
    "\n",
    "        # Effectiveness Task\n",
    "        key_effectiveness = (id_str, 'effectiveness', response_A, response_B)\n",
    "        prompt_effectiveness = compose_prompt_ranking_effectiveness(text, response_A, response_B)\n",
    "        if key_effectiveness not in evaluation_dict or evaluation_dict[key_effectiveness] is None:\n",
    "            tasks.append((key_effectiveness, prompt_effectiveness))\n",
    "            \n",
    "# Log\n",
    "print(f\"Generated {len(tasks):,} tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_MAP = [\n",
    "    ('A', 0),\n",
    "    ('B', 1)\n",
    "]\n",
    "NUM_WORKERS = 10\n",
    "FILEPATH = filepath_evaluations\n",
    "JSON_MODE = False\n",
    "MODEL_LARGE = True\n",
    "\n",
    "# Run the function\n",
    "parallel_process_batches(\n",
    "    tasks=tasks,\n",
    "    filepath=FILEPATH, \n",
    "    results_map=RESULTS_MAP, \n",
    "    json_mode=JSON_MODE,\n",
    "    model_large=MODEL_LARGE, \n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Collect all evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Categories\n",
    "RESPONSE_GENERATION_CATEGORIES = [ 'control', 'generic', 'RAG', 'community_note', 'community_note_RAG' ]\n",
    "\n",
    "\"\"\" Step 1: Load Data \"\"\"\n",
    "\n",
    "# Load data\n",
    "responses_dict = load_dict_from_json(filepath_responses)\n",
    "evaluation_dict = load_dict_from_json(filepath_evaluations)\n",
    "\n",
    "# Map llm responses to response category\n",
    "map_response_to_category = {}\n",
    "for (document_id, response_category, idx), response in responses_dict.items():\n",
    "    map_response_to_category[remove_links(response)] = response_category\n",
    "\n",
    "# Load dyads\n",
    "with open(filepath_dyads, 'r') as file:\n",
    "    dyads = json.load(file)\n",
    "\n",
    "# Map community notes to response category\n",
    "for document in dyads:\n",
    "    summary = remove_links(document['note'])\n",
    "    map_response_to_category[summary] = 'community_note'\n",
    "\n",
    "\n",
    "\"\"\" Step 2: Format Comparisons \"\"\"\n",
    "\n",
    "# Initialize\n",
    "evaluations = []\n",
    "\n",
    "# Convert comparisons format\n",
    "for (document_id, evaluation_category, response_A, response_B), result in evaluation_dict.items():\n",
    "\n",
    "    # Map responses to their categories\n",
    "    try:\n",
    "        category_A = map_response_to_category[response_A]\n",
    "        category_B = map_response_to_category[response_B]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Get winning category\n",
    "    if result == 0:\n",
    "        category_winning = category_A\n",
    "    elif result == 1:\n",
    "        category_winning = category_B\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Append\n",
    "    evaluations.append((document_id, category_A, category_B, category_winning))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Step 3: Aggregate by document \"\"\"\n",
    "\n",
    "# Initialize\n",
    "evaluations_by_document = { document_id: [] for document_id, _, _, _ in evaluations }\n",
    "\n",
    "# Populate\n",
    "for document_id, category_A, category_B, category_winning in evaluations:\n",
    "    evaluations_by_document[document_id].append((category_A, category_B, category_winning))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Quality Assurance\n",
    "\n",
    "We only keep the evaluations where the last category is 'control' or the last two categories are 'community_note' and 'control'.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize\n",
    "evaluations_by_document_valid = {}\n",
    "\n",
    "# Iterate over\n",
    "for document_id, document_evaluations in evaluations_by_document.items():\n",
    "\n",
    "    # Skip if empty\n",
    "    if len(document_evaluations) == 0: continue\n",
    "\n",
    "    # Initialize\n",
    "    category_winning_counts = { category: { 'win': 0, 'total': 0 } for category in RESPONSE_GENERATION_CATEGORIES }\n",
    "\n",
    "    # Iterate over\n",
    "    for category_A, category_B, category_winning in document_evaluations:\n",
    "        category_winning_counts[category_A]['total'] += 1\n",
    "        category_winning_counts[category_B]['total'] += 1\n",
    "        category_winning_counts[category_winning]['win'] += 1\n",
    "\n",
    "    # Compute winning percentages\n",
    "    for category in RESPONSE_GENERATION_CATEGORIES:\n",
    "        total = category_winning_counts[category]['total']\n",
    "        win = category_winning_counts[category]['win']\n",
    "        category_winning_counts[category]['win_%'] = round(win / total, 3) if total > 0 else 0\n",
    "\n",
    "    # Sort by winning percentage\n",
    "    category_winning_counts = { k: v for k, v in sorted(category_winning_counts.items(), key=lambda item: item[1]['win_%'], reverse=True) }\n",
    "\n",
    "    # Get ranking\n",
    "    ranking = [ category for category in category_winning_counts.keys() ]\n",
    "\n",
    "    # # Discard if invalid under QA\n",
    "    # if ranking[-1] != 'control' and ranking[-1] != 'generic': continue\n",
    "\n",
    "    # Store\n",
    "    evaluations_by_document_valid[document_id] = ranking\n",
    "\n",
    "# Log\n",
    "print(f\"Filtered {len(evaluations_by_document):,} -> {len(evaluations_by_document_valid):,} documents.\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Step 5: Compute Overall Ranking, and Statistics \"\"\"\n",
    "\n",
    "# Initialize overall statistics\n",
    "category_statistics = { category: {'win': 0, 'total': 0, 'win_%': 0} for category in RESPONSE_GENERATION_CATEGORIES }\n",
    "\n",
    "# Aggregate statistics across all documents\n",
    "for document_id, rankings in evaluations_by_document_valid.items():\n",
    "    for i, category in enumerate(rankings):\n",
    "        category_statistics[category]['total'] += 1\n",
    "        if i == 0:  # Top rank counts as a win\n",
    "            category_statistics[category]['win'] += 1\n",
    "\n",
    "# Compute win percentages\n",
    "for category, stats in category_statistics.items():\n",
    "    total = stats['total']\n",
    "    win = stats['win']\n",
    "    stats['win_%'] = round(win / total, 3) if total > 0 else 0\n",
    "\n",
    "# Sort categories by win percentage\n",
    "overall_ranking = [category for category, stats in sorted(category_statistics.items(), key=lambda x: x[1]['win_%'], reverse=True)]\n",
    "\n",
    "# Log results\n",
    "print(\"\\nOverall Ranking by Win Percentage:\")\n",
    "print(\"-----------------------------------\")\n",
    "for category in overall_ranking:\n",
    "    print(f\"{category.ljust(24)} {category_statistics[category]['win_%']:.3f} (Wins: {category_statistics[category]['win']}, Total: {category_statistics[category]['total']})\")\n",
    "\n",
    "\n",
    "\"\"\" Step 6: Compute Friedman-like Test \"\"\"\n",
    "\n",
    "# Prepare data for Friedman-like test (ranking significance)\n",
    "evaluation_matrix = []\n",
    "for document_id, rankings in evaluations_by_document_valid.items():\n",
    "    document_ranks = [RESPONSE_GENERATION_CATEGORIES.index(category) for category in rankings]\n",
    "    evaluation_matrix.append(document_ranks)\n",
    "\n",
    "evaluation_matrix = np.array(evaluation_matrix)\n",
    "\n",
    "# Perform Friedman-like test manually\n",
    "if evaluation_matrix.shape[0] > 1:\n",
    "    k = evaluation_matrix.shape[1]  # Number of categories\n",
    "    n = evaluation_matrix.shape[0]  # Number of documents\n",
    "\n",
    "    # Compute rank sums for each category\n",
    "    rank_sums = [0] * k\n",
    "    for row in evaluation_matrix:\n",
    "        for i, rank in enumerate(row):\n",
    "            rank_sums[i] += rank\n",
    "\n",
    "    # Compute test statistic\n",
    "    chi_square = (12 / (n * k * (k + 1))) * sum((rank_sum - n * (k + 1) / 2) ** 2 for rank_sum in rank_sums)\n",
    "    p_value = 1 - (1 if chi_square > 10 else 0)  # Simplified p-value approximation\n",
    "\n",
    "    print(f\"\\nFriedman-like Test Results: Chi-square={chi_square:.3f}, p-value={p_value:.3f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"The differences among the categories are statistically significant.\")\n",
    "    else:\n",
    "        print(\"No statistically significant differences among the categories.\")\n",
    "\n",
    "\n",
    "\"\"\" Step 6: Compute Confidence Intervals for Win Rates \"\"\"\n",
    "\n",
    "# Compute Confidence Intervals for Win Rates\n",
    "confidence_intervals = {}\n",
    "def bootstrap_mean(data, n_resamples=1000):\n",
    "    means = []\n",
    "    for _ in range(n_resamples):\n",
    "        sample = [data[i] for i in np.random.randint(0, len(data), len(data))]\n",
    "        means.append(sum(sample) / len(sample))\n",
    "    means.sort()\n",
    "    return round(means[int(0.025 * len(means))], 3), round(means[int(0.975 * len(means))], 3)\n",
    "\n",
    "for category in RESPONSE_GENERATION_CATEGORIES:\n",
    "    win_counts = category_statistics[category]['win']\n",
    "    total_counts = category_statistics[category]['total']\n",
    "    if total_counts > 0:\n",
    "        data = [1] * win_counts + [0] * (total_counts - win_counts)\n",
    "        confidence_intervals[category] = bootstrap_mean(data)\n",
    "    else:\n",
    "        confidence_intervals[category] = (0, 0)\n",
    "\n",
    "# Sort\n",
    "confidence_intervals = { k: v for k, v in sorted(confidence_intervals.items(), key=lambda item: item[1][0], reverse=True) }\n",
    "\n",
    "print(\"\\nConfidence Intervals for Win Rates:\")\n",
    "print(\"---------------------------------------\")\n",
    "for category, ci in confidence_intervals.items():\n",
    "    print(f\"{category.ljust(24)} {ci}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
